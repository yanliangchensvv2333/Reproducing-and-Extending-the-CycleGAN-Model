{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/zhisheng-hua/Reproducing-and-Extending-the-CycleGAN-Model/blob/cy/CycleGAN_Draft1.ipynb",
      "authorship_tag": "ABX9TyOx0rBcbEeoIcMiptnGlNTr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhisheng-hua/Reproducing-and-Extending-the-CycleGAN-Model/blob/cy/CycleGAN_Draft1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "TSEh-iK-I7d7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j-OVhbBGfISI"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import os \n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "# from tqdm import tqdm\n",
        "# import albumentations as A\n",
        "# from albumentations.pytorch import ToTensor\n",
        "from torch.types import Number\n",
        "import sys\n",
        "# from torch.utils.data import DataLoader\n",
        "# import torch.optim as optim\n",
        "# from tqdm import tqdm\n",
        "from torchvision.utils import save_image\n",
        "# import albumentations as A\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import time\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "6bVWAUIxVojW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)"
      ],
      "metadata": {
        "id": "-vxmIPcJ3KLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d80b2d4d-26e3-47fd-b94a-30101f2e5c3e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 1e-5\n",
        "LAMBDA_IDT = 10\n",
        "LAMBDA_CYCLE = 5\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "LOAD_MODEL=False\n",
        "SAVE_MODEL=False"
      ],
      "metadata": {
        "id": "lKEvvEdRVn3V"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "7XlppgzSw4uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n"
      ],
      "metadata": {
        "id": "_MDUWpbIw4UW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "xorqpTpAV18P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataSet(Dataset):\n",
        "    def __init__(self, A_dir, B_dir, transform, random_pairs=False):\n",
        "        self.A_dir = A_dir\n",
        "        self.B_dir = B_dir\n",
        "        self.transform = transform\n",
        "        self.random_pairs = random_pairs\n",
        "\n",
        "        self.A_img_filenames = os.listdir(self.A_dir)\n",
        "        self.B_img_filenames = os.listdir(self.B_dir)\n",
        "        self.A_size = len(self.A_img_filenames)\n",
        "        self.B_size = len(self.B_img_filenames)\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(self.A_size, self.B_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        A_img_path = os.path.join(self.A_dir,\n",
        "                                  self.A_img_filenames[index % self.A_size])\n",
        "        if self.random_pairs:\n",
        "          B_index = random.randint(0, self.B_size - 1)\n",
        "        else:\n",
        "          B_index = index % self.B_size\n",
        "        B_img_path = os.path.join(self.B_dir, self.B_img_filenames[B_index])\n",
        "\n",
        "        A_img = self.transform(Image.open(A_img_path))\n",
        "        B_img = self.transform(Image.open(B_img_path))\n",
        "\n",
        "        return A_img, B_img"
      ],
      "metadata": {
        "id": "TNYKRpXTV6GP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir ./Segment ./Ground"
      ],
      "metadata": {
        "id": "32409u2-9Dka"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = '/content/drive/MyDrive/ECE-GY 6953 DL/Project/Datasets/mini/trainA'\n",
        "path2 = '/content/drive/MyDrive/ECE-GY 6953 DL/Project/Datasets/mini/trainB'"
      ],
      "metadata": {
        "id": "In8OEjAEuSu-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image transformation\n",
        "\n",
        "train_data_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "test_data_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "e9J4e-ONnS11"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for input\n",
        "train_dataset = CustomDataSet(path1, path2, train_data_transform)\n",
        "# TODO: test_dataset\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                batch_size=BATCH_SIZE,\n",
        "                                                shuffle=True,\n",
        "                                                num_workers=NUM_WORKERS)\n",
        "# TODO: test_data_loader"
      ],
      "metadata": {
        "id": "-YQFxRVa3vXe",
        "outputId": "700034a9-5b20-4b78-a6b7-96ca928e9495",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator\n",
        "TODO Modify the code"
      ],
      "metadata": {
        "id": "YqZAcAJJSXEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, padding, down=True, use_act=True, **kwargs):\n",
        "        super().__init__()\n",
        "        if down:\n",
        "          self.conv1 = nn.Conv2d(in_channel, out_channel, padding_mode=\"reflect\", kernel_size=kernel_size, padding=padding, **kwargs)\n",
        "        else:\n",
        "          self.conv1 = nn.ConvTranspose2d(in_channel, out_channel, kernel_size=kernel_size, padding=padding, **kwargs)\n",
        "        self.IN = nn.InstanceNorm2d(out_channel)\n",
        "        if use_act:\n",
        "          self.acv = nn.ReLU(inplace=True)\n",
        "        else:\n",
        "          self.acv = nn.Identity()\n",
        "    def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.IN(x)\n",
        "      return  self.acv(x)"
      ],
      "metadata": {
        "id": "KMhkBIsOSWZx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, channels):\n",
        "    super().__init__()\n",
        "    self.Conv1 = ConvBlock(channels, channels, kernel_size=3, padding=1)\n",
        "    self.Conv2 = ConvBlock(channels, channels, kernel_size=3, padding=1, use_act=False)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x2 = self.Conv1(x)\n",
        "    x2 = self.Conv2(x2)\n",
        "    return x + x2"
      ],
      "metadata": {
        "id": "u6cqvJqZuvW1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, img_channels, num_features=64, num_residual=9):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n",
        "    self.acv1 = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.DownBlock = nn.ModuleList([\n",
        "        nn.Conv2d(num_features, num_features*2, kernel_size=3, stride=2, padding=1),\n",
        "        nn.Conv2d(num_features*2, num_features*4, kernel_size=3, stride=2, padding=1),\n",
        "    ])\n",
        "\n",
        "    self.residual_blocks = nn.Sequential(\n",
        "        *[ResBlock(num_features*4) for _ in range(num_residual)]\n",
        "    )\n",
        "\n",
        "    self.up_blocks = nn.ModuleList([\n",
        "        ConvBlock(num_features*4, num_features*2, kernel_size=3, stride=2, padding=1, output_padding=1, down=False),\n",
        "        ConvBlock(num_features*2, num_features*1, kernel_size=3, stride=2, padding=1, output_padding=1, down=False),\n",
        "    ])\n",
        "\n",
        "    self.last_layer = nn.Conv2d(num_features, img_channels, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.acv1(x)\n",
        "    for layer in self.DownBlock:\n",
        "      x = layer(x)\n",
        "\n",
        "    for layer in self.up_blocks:\n",
        "      x = layer(x)\n",
        "    return torch.tanh(self.last_layer(x))\n",
        "    \n"
      ],
      "metadata": {
        "id": "bd4faoHsux50"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator\n",
        "TODO Modify the code"
      ],
      "metadata": {
        "id": "nBs6pDsiSd-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscBlock(nn.Module):\n",
        "  def __init__(self, in_channel, out_channel, stride):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channel, out_channel, 4, stride, 1, bias=True, padding_mode=\"reflect\")\n",
        "    self.IN1 = nn.InstanceNorm2d(out_channel)\n",
        "    self.act1 = nn.LeakyReLU(0.2)\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.IN1(x)\n",
        "    x = self.act1(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "RydVOs8Cgcmm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channel, num_features=[64, 128, 256, 512]):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channel, num_features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\")\n",
        "    self.act1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "    layers = []\n",
        "    in_channel = num_features[0]\n",
        "    \n",
        "    for counter in range(1,len(num_features)):\n",
        "\n",
        "      layers.append(DiscBlock(in_channel, num_features[counter], stride=1 if counter==(len(num_features)-1) else 2))\n",
        "      in_channel = num_features[counter]\n",
        "    layers.append(nn.Conv2d(in_channel, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.act1(x)\n",
        "    x = self.model(x)\n",
        "    return torch.sigmoid(x)\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "M8T-FBTESrPw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Y_tLfkrbvZp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to train for one epoch\n",
        "def train_CycleGAN_for_one_epoch(G_A, D_A, G_B, D_B,\n",
        "                                 cri_GAN, cri_Cycle, cri_Idt,\n",
        "                                 opt_G, opt_D,\n",
        "                                 data_loader):\n",
        "  G_A.train()\n",
        "  D_A.train()\n",
        "  G_B.train()\n",
        "  D_B.train()\n",
        "\n",
        "  # TODO: modify shape\n",
        "  D_labels_real = torch.ones([BATCH_SIZE,1,30,30]).to(DEVICE)\n",
        "  D_labels_fake = torch.zeros([BATCH_SIZE,1,30,30]).to(DEVICE)\n",
        "\n",
        "  for i, (A_real, B_real) in enumerate(data_loader):\n",
        "    A_real = A_real.to(DEVICE)\n",
        "    B_real = B_real.to(DEVICE)\n",
        "\n",
        "    if A_real.shape[0] < BATCH_SIZE:\n",
        "      D_labels_real = D_labels_real[:A_real.shape[0]]\n",
        "      D_labels_fake = D_labels_fake[:A_real.shape[0]]\n",
        "\n",
        "    # forward pass; generate fake images\n",
        "    B_fake = G_A(A_real) # G_A(A)\n",
        "    A_fake = G_B(B_real) # G_B(B)\n",
        "\n",
        "    ## Update generators\n",
        "    opt_G.zero_grad()\n",
        "    # GAN losses\n",
        "    loss_GAN_G_A = cri_GAN(D_A(B_fake), D_labels_real)\n",
        "    loss_GAN_G_B = cri_GAN(D_B(A_fake), D_labels_real)\n",
        "    # Cycle losses\n",
        "    loss_Cycle_G_A = cri_Cycle(G_B(B_fake), A_real) # loss(G_B(G_A(A)), A)\n",
        "    loss_Cycle_G_B = cri_Cycle(G_A(A_fake), B_real) # loss(G_A(G_B(B)), B)\n",
        "    # Identity losses\n",
        "    loss_Idt_G_A = cri_Idt(G_A(B_real), B_real) # loss(G_A(B), B)\n",
        "    loss_Idt_G_B = cri_Idt(G_B(A_real), A_real) # loss(G_B(A), A)\n",
        "    # Combine all losses\n",
        "    loss_G = (loss_GAN_G_A + loss_GAN_G_B\n",
        "              + LAMBDA_CYCLE * (loss_Cycle_G_A + loss_Cycle_G_B)\n",
        "              + LAMBDA_IDT * (loss_Idt_G_A + loss_Idt_G_B))\n",
        "    # Update\n",
        "    loss_G.backward()\n",
        "    opt_G.step()\n",
        "\n",
        "    ## Update discriminators\n",
        "    opt_D.zero_grad()\n",
        "    # Update D_A (discriminator for A->B)\n",
        "    # Real images\n",
        "    loss_D_A_real = cri_GAN(D_A(B_real), D_labels_real)\n",
        "    # Fake images\n",
        "    loss_D_A_fake = cri_GAN(D_A(B_fake.detach()), D_labels_fake)\n",
        "    # Combine losses and compute gradients\n",
        "    loss_D_A = (loss_D_A_real + loss_D_A_fake) * 0.5\n",
        "    loss_D_A.backward()\n",
        "    # Update D_B (discriminator for B->A)\n",
        "    # Real images\n",
        "    loss_D_B_real = cri_GAN(D_B(A_real), D_labels_real)\n",
        "    # Fake images\n",
        "    loss_D_B_fake = cri_GAN(D_B(A_fake.detach()), D_labels_fake)\n",
        "    # Combine losses and compute gradients\n",
        "    loss_D_B = (loss_D_B_real + loss_D_B_fake) * 0.5\n",
        "    loss_D_B.backward()\n",
        "    # Update the optimizer\n",
        "    opt_D.step()"
      ],
      "metadata": {
        "id": "2HGs3VEMhiSe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training setup\n",
        "\n",
        "# Generator and Discriminator models\n",
        "G_A = Generator(img_channels=3, num_residual=9).to(DEVICE)\n",
        "D_A = Discriminator(in_channel=3).to(DEVICE)\n",
        "G_B = Generator(img_channels=3, num_residual=9).to(DEVICE)\n",
        "D_B = Discriminator(in_channel=3).to(DEVICE)\n",
        "\n",
        "# Losses\n",
        "cri_GAN = nn.MSELoss() # Least-Squares GAN; see Reference 35 of the paper\n",
        "cri_Cycle = nn.L1Loss()\n",
        "cri_Idt = nn.L1Loss()\n",
        "\n",
        "# Optimizers\n",
        "opt_G = torch.optim.Adam(itertools.chain(G_A.parameters(), G_B.parameters()),\n",
        "                         lr=LR, betas=(0.5, 0.999))\n",
        "opt_D = torch.optim.Adam(itertools.chain(D_A.parameters(), D_B.parameters()),\n",
        "                         lr=LR, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "jm-YnUgjyVR7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for all epoches\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(\"Epoch: \", epoch)\n",
        "  s_time = time.time_ns()\n",
        "\n",
        "  # Training process\n",
        "  train_CycleGAN_for_one_epoch(G_A, D_A, G_B, D_B,\n",
        "                               cri_GAN, cri_Cycle, cri_Idt,\n",
        "                               opt_G, opt_D,\n",
        "                               train_data_loader)\n",
        "  \n",
        "  e_time = time.time_ns()\n",
        "  print(\"  Time elapsed: {} min\".format((e_time - s_time) / 1e9 / 60))"
      ],
      "metadata": {
        "id": "SKNcY7VNyvWu",
        "outputId": "256dfe42-79f7-414c-85f0-76ea83502a7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0\n",
            "  Time elapsed: 0.01541389385 min\n",
            "Epoch:  1\n",
            "  Time elapsed: 0.0305093105 min\n",
            "Epoch:  2\n",
            "  Time elapsed: 0.03222081765 min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def TrainCicleGAN(Disc_Ground, Gen_Seg, Disc_Seg, Gen_Ground, loader, optim_Disc, optim_Gen, L1, MSE, Disc_scaler, Gen_scaler):\n",
        "#   # loop_prop = tqdm(loader, leave=True)\n",
        "#   index2 = 0\n",
        "#   # for index, (segm, grd) in enumerate(loop_prop):\n",
        "#   for index, (segm, grd) in enumerate(loader):\n",
        "#     grd = grd.to(DEVICE)\n",
        "#     segm = segm.to(DEVICE)\n",
        "\n",
        "#     H_reals = 0\n",
        "#     H_fakes = 0\n",
        "#     with torch.cuda.amp.autocast():\n",
        "#         print(\"test\")\n",
        "#         fake_ground = Gen_Ground(segm)\n",
        "#         D_H_real = Disc_Ground(grd)\n",
        "#         D_H_fake = Disc_Ground(fake_ground.detach())\n",
        "#         H_reals += D_H_real.mean().item()\n",
        "#         H_fakes += D_H_fake.mean().item()\n",
        "#         D_H_real_loss = MSE(D_H_real, torch.ones_like(D_H_real))\n",
        "#         D_H_fake_loss = MSE(D_H_fake, torch.zeros_like(D_H_fake))\n",
        "#         D_H_loss = D_H_real_loss + D_H_fake_loss\n",
        "\n",
        "#         print(torch.ones_like(D_H_real).shape)\n",
        "#         print(torch.ones_like(D_H_real))\n",
        "       \n",
        "#         fake_seg = Gen_Seg(grd)\n",
        "#         D_Z_real = Disc_Seg(segm)\n",
        "#         D_Z_fake = Disc_Seg(fake_seg.detach())\n",
        "#         D_Z_real_loss = MSE(D_Z_real, torch.ones_like(D_Z_real))\n",
        "#         D_Z_fake_loss = MSE(D_Z_fake, torch.zeros_like(D_Z_fake))\n",
        "#         D_Z_loss = D_Z_real_loss + D_Z_fake_loss\n",
        "\n",
        "#         # put it togethor\n",
        "#         D_loss = (D_H_loss + D_Z_loss)/2\n",
        "\n",
        "#     optim_Disc.zero_grad()\n",
        "#     Disc_scaler.scale(D_loss).backward()\n",
        "#     Disc_scaler.step(optim_Disc)\n",
        "#     Disc_scaler.update()\n",
        "\n",
        "#     # Train Generators H and Z\n",
        "#     with torch.cuda.amp.autocast():\n",
        "#         # adversarial loss for both generators\n",
        "#         D_H_fake = Disc_Ground(fake_ground)\n",
        "#         D_Z_fake = Disc_Seg(fake_seg)\n",
        "#         loss_G_H = MSE(D_H_fake, torch.ones_like(D_H_fake))\n",
        "#         loss_G_Z = MSE(D_Z_fake, torch.ones_like(D_Z_fake))\n",
        "\n",
        "#         # cycle loss\n",
        "#         cycle_segm = Gen_Seg(fake_ground)\n",
        "#         cycle_ground = Gen_Ground(fake_seg)\n",
        "#         cycle_segm_loss = L1(segm, cycle_segm)\n",
        "#         cycle_ground_loss = L1(grd, cycle_ground)\n",
        "\n",
        "#         # identity loss (remove these for efficiency if you set lambda_identity=0)\n",
        "#         identity_segm = Gen_Seg(segm)\n",
        "#         identity_ground = Gen_Ground(grd)\n",
        "#         identity_segm_loss = L1(segm, identity_segm)\n",
        "#         identity_ground_loss = L1(grd, identity_ground)\n",
        "\n",
        "#         # add all togethor\n",
        "#         G_loss = (\n",
        "#             loss_G_Z\n",
        "#             + loss_G_H\n",
        "#             + cycle_segm_loss * LAMBDA_CYCLE\n",
        "#             + cycle_ground_loss * LAMBDA_CYCLE\n",
        "#             # + identity_ground_loss * LAMBDA_IDENTITY\n",
        "#             # + identity_segm_loss * LAMBDA_IDENTITY\n",
        "#         )\n",
        "\n",
        "#     optim_Gen.zero_grad()\n",
        "#     Gen_scaler.scale(G_loss).backward()\n",
        "#     Gen_scaler.step(optim_Gen)\n",
        "#     Gen_scaler.update()\n",
        "\n",
        "#     # if index2 % 200 == 0:\n",
        "#         # save_image(fake_ground, SAVE_IMAGE + f\"/Ground/{index2}.png\")\n",
        "#         # save_image(fake_seg, SAVE_IMAGE + f\"/Segment/{index2}.png\")\n",
        "#     save_image(fake_ground, f\"./Ground/{index2}.png\")\n",
        "#     save_image(fake_seg, f\"./Segment/{index2}.png\")\n",
        "#     index2 += 1\n",
        "#     loop_prop.set_postfix(H_real=H_reals/(index+1), H_fake=H_fakes/(index+1))\n",
        "\n",
        "#   return fake_ground, fake_seg\n",
        "\n"
      ],
      "metadata": {
        "id": "EpIT6DOmcjQY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Function"
      ],
      "metadata": {
        "id": "DjLn1BDan328"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchvision import datasets"
      ],
      "metadata": {
        "id": "8IOvnSTYGI1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "Disc_Ground = Discriminator(in_channel=3).to(DEVICE)\n",
        "Disc_Seg = Discriminator(in_channel=3).to(DEVICE)\n",
        "Gen_Ground = Generator(img_channels=3, num_residual=9).to(DEVICE)\n",
        "Gen_Seg = Generator(img_channels=3, num_residual=9).to(DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "optim_Disc = torch.optim.Adam(list(Disc_Ground.parameters()) + list(Disc_Seg.parameters()), lr=LR, betas=(0.5, 0.999),)\n",
        "optim_Gen = torch.optim.Adam(list(Disc_Ground.parameters()) + list(Gen_Seg.parameters()), lr=LR, betas=(0.5, 0.999),)\n",
        "\n",
        "L1 = nn.L1Loss()\n",
        "MSE = nn.MSELoss()\n",
        "\n",
        "\n",
        "# if LOAD_MODEL:\n",
        "\n",
        "#   Disc_Ground.load_state_dict(torch.load(WEIGHTS_DISC_G))\n",
        "#   Disc_Seg.load_state_dict(torch.load(WEIGHTS_DISC_S))\n",
        "#   Gen_Ground.load_state_dict(torch.load(WEIGHTS_GEN_G))\n",
        "#   Gen_Seg.load_state_dict(torch.load(WEIGHTS_GEN_S))\n",
        "#     # load_checkpoint(\n",
        "#     #     WEIGHTS_GEN_G, Gen_Ground, optim_Gen, LEARNING_RATE,\n",
        "#     # )\n",
        "#     # load_checkpoint(\n",
        "#     #     WEIGHTS_GEN_S, Gen_Seg, optim_Gen, LEARNING_RATE,\n",
        "#     # )\n",
        "#     # load_checkpoint(\n",
        "#     #     WEIGHTS_DISC_G, Disc_Ground, optim_Disc, LEARNING_RATE,\n",
        "#     # )\n",
        "#     # load_checkpoint(\n",
        "#     #     WEIGHTS_DISC_S, Disc_Seg, optim_Disc, LEARNING_RATE,\n",
        "#     # )\n",
        "# dataset = CityscapesDataSet(SEGMENT_PATH, GROUND_PATH, transforms2)\n",
        "# test_dataset = CityscapesDataSet(SEGMENT_PATH_TEST, GROUND_PATH_TEST, transforms2)\n",
        "dataset = CustomDataSet(path1, path2, train_data_transform)\n",
        "# test_dataset = CityscapesDataSet(SEGMENT_PATH_TEST, GROUND_PATH_TEST, transforms2)\n",
        "\n",
        "loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# test_loader = DataLoader(\n",
        "#   test_dataset,\n",
        "#   batch_size=1,\n",
        "#   shuffle=False,\n",
        "#   pin_memory=True,\n",
        "# )\n",
        "\n",
        "Gen_scaler = torch.cuda.amp.GradScaler()\n",
        "Disc_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    s_time = time.time_ns()\n",
        "    x1, x2 = TrainCicleGAN(Disc_Ground, Gen_Seg, Disc_Seg, Gen_Ground, loader, optim_Disc, optim_Gen, L1, MSE, Disc_scaler, Gen_scaler)\n",
        "    e_time = time.time_ns()\n",
        "    print(\"Time elapsed: {} min\".format((e_time - s_time) / 1e9 / 60))\n",
        "    print()\n",
        "\n",
        "    # if SAVE_MODEL:\n",
        "    #   torch.save(Disc_Ground.state_dict(), WEIGHTS_DISC_G)\n",
        "    #   torch.save(Disc_Seg.state_dict(), WEIGHTS_DISC_S)\n",
        "\n",
        "    #   torch.save(Gen_Ground.state_dict(), WEIGHTS_GEN_G)\n",
        "    #   torch.save(Gen_Seg.state_dict(), WEIGHTS_GEN_S)\n",
        "\n"
      ],
      "metadata": {
        "id": "z66W2LNhvZje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmpimg = Image.open('/content/drive/MyDrive/ECE-GY 6953 DL/Project/Datasets/mini/trainA/n02381460_1127.jpg')\n",
        "# tmpimg = train_data_transform(tmpimg).permute(1,2,0)\n",
        "plt.imshow(tmpimg)"
      ],
      "metadata": {
        "id": "1p4Yx0XXSqgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/Segment/0.png'\n",
        "# path = '/content/drive/MyDrive/ECE-GY 6953 DL/Project/Datasets/mini/trainA/n02381460_1127.jpg'\n",
        "img = Image.open(path)\n",
        "print(np.array(img).shape)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "GwspE86JAcPz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}