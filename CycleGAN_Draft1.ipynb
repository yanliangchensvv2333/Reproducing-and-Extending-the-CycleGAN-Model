{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "toc_visible": true,
      "collapsed_sections": [
        "YqZAcAJJSXEf",
        "nBs6pDsiSd-Y"
      ],
      "mount_file_id": "https://github.com/zhisheng-hua/Reproducing-and-Extending-the-CycleGAN-Model/blob/cy/CycleGAN_Draft1.ipynb",
      "authorship_tag": "ABX9TyNZcMB0TgEl81QvPOEFwT0N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhisheng-hua/Reproducing-and-Extending-the-CycleGAN-Model/blob/main/CycleGAN_Draft1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ],
      "metadata": {
        "id": "TSEh-iK-I7d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dCvQHGCSJfGL",
        "outputId": "98aa367e-5b8e-409b-8a7c-c8261a45def6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j-OVhbBGfISI"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.utils import save_image\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters and Constant Variables"
      ],
      "metadata": {
        "id": "6bVWAUIxVojW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)"
      ],
      "metadata": {
        "id": "-vxmIPcJ3KLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d7664d0-af90-466c-8e3e-0f7a3450fe28"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DIR = \"/content/drive/MyDrive/ECE-GY 6953 DL/Project/Datasets/horse2zebra/\"\n",
        "TRAIN_A_PATH = DATASET_DIR + \"trainA\"\n",
        "TRAIN_B_PATH = DATASET_DIR + \"trainA\"\n",
        "TEST_A_PATH = DATASET_DIR + \"testA\"\n",
        "TEST_B_PATH = DATASET_DIR + \"testA\"\n",
        "\n",
        "MODEL_DIR = \"/content/drive/MyDrive/ECE-GY 6953 DL/Project/SavedModels/\"\n",
        "\n",
        "VISUAL_DIR = \"/content/drive/MyDrive/ECE-GY 6953 DL/Project/Visualizations/\""
      ],
      "metadata": {
        "id": "LD1k80gisQqY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 100\n",
        "NUM_EPOCHS_DECAY = 0 # the number of last epoches during which lr decays to 0\n",
        "\n",
        "LR = 0.0002\n",
        "LAMBDA_CYCLE = 10.0\n",
        "LAMBDA_IDT = 0.5\n",
        "IMAGE_POOL_SIZE = 50\n",
        "\n",
        "MAX_NUM_SAMPLES = 1000 # if larger than the number of training/testing samples, use all samples\n",
        "NUM_DATALOADER_WORKERS = 2\n",
        "SAVE_MODELS = True\n",
        "SAVE_LOSS_PLOTS = False"
      ],
      "metadata": {
        "id": "lKEvvEdRVn3V"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "7XlppgzSw4uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The code in this cell is adapted from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git.\n",
        "\n",
        "# Method to obtain/insert images from/into the image pool\n",
        "def query_image_pool(image_pool, pool_size, curr_images):\n",
        "  if pool_size == 0:\n",
        "      return curr_images\n",
        "  ret_images = []\n",
        "  for image in curr_images:\n",
        "      image = torch.unsqueeze(image.data, 0)\n",
        "      if len(image_pool) < pool_size: # if buffer is not full\n",
        "          image_pool.append(image)\n",
        "          ret_images.append(image)\n",
        "      else: # if buffer is full\n",
        "          # by 50% chance, return an old image from the pool and replace it with\n",
        "          # a current image; otherwise, return the current image\n",
        "          p = random.uniform(0, 1)\n",
        "          if p > 0.5:\n",
        "              random_id = random.randint(0, pool_size - 1)\n",
        "              old_image = image_pool[random_id].clone()\n",
        "              image_pool[random_id] = image\n",
        "              ret_images.append(old_image)\n",
        "          else:\n",
        "              ret_images.append(image)\n",
        "  return torch.cat(ret_images, 0)"
      ],
      "metadata": {
        "id": "rR7l18OIBsCD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code in this cell is adapted from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git.\n",
        "\n",
        "# Method to initialize weights of the given model\n",
        "def init_weights(model):\n",
        "  def init_func(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "      init.normal_(m.weight.data, 0.0, 0.02)\n",
        "      if hasattr(m, 'bias') and m.bias is not None:\n",
        "          init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "      init.normal_(m.weight.data, 1.0, 0.02)\n",
        "      init.constant_(m.bias.data, 0.0)\n",
        "  model.apply(init_func)"
      ],
      "metadata": {
        "id": "XeEC6hCH9A8o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code in this cell is adapted from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git.\n",
        "\n",
        "# Method to post-processing the output image of the generator\n",
        "def post_processing_image(image):\n",
        "  image = image.cpu().float().numpy()\n",
        "  image_np = (np.transpose(image, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "  return image_np.astype(np.uint8)"
      ],
      "metadata": {
        "id": "_0gl56YpxCb0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "xorqpTpAV18P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, A_dir, B_dir, transform, random_pairs=True):\n",
        "        self.A_dir = A_dir\n",
        "        self.B_dir = B_dir\n",
        "        self.transform = transform\n",
        "        self.random_pairs = random_pairs\n",
        "\n",
        "        self.A_img_filenames = os.listdir(self.A_dir)[:MAX_NUM_SAMPLES]\n",
        "        self.B_img_filenames = os.listdir(self.B_dir)[:MAX_NUM_SAMPLES]\n",
        "        self.A_size = len(self.A_img_filenames)\n",
        "        self.B_size = len(self.B_img_filenames)\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(self.A_size, self.B_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        A_img_path = os.path.join(self.A_dir,\n",
        "                                  self.A_img_filenames[index % self.A_size])\n",
        "        if self.random_pairs:\n",
        "          B_index = random.randint(0, self.B_size - 1)\n",
        "        else:\n",
        "          B_index = index % self.B_size\n",
        "        B_img_path = os.path.join(self.B_dir, self.B_img_filenames[B_index])\n",
        "\n",
        "        A_img = self.transform(Image.open(A_img_path).convert(\"RGB\"))\n",
        "        B_img = self.transform(Image.open(B_img_path).convert(\"RGB\"))\n",
        "\n",
        "        return A_img, B_img"
      ],
      "metadata": {
        "id": "TNYKRpXTV6GP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image transformation\n",
        "train_data_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "test_data_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "e9J4e-ONnS11"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup datasets and dataloaders\n",
        "train_dataset = CustomDataset(TRAIN_A_PATH, TRAIN_B_PATH, train_data_transform,\n",
        "                              random_pairs=True)\n",
        "test_dataset = CustomDataset(TEST_A_PATH, TEST_B_PATH, test_data_transform,\n",
        "                             random_pairs=False)\n",
        "print(\"Number of training samples:\", len(train_dataset))\n",
        "print(\"Number of testing samples:\", len(train_dataset))\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                batch_size=BATCH_SIZE,\n",
        "                                                shuffle=True,\n",
        "                                                num_workers=NUM_DATALOADER_WORKERS)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=NUM_DATALOADER_WORKERS)"
      ],
      "metadata": {
        "id": "-YQFxRVa3vXe",
        "outputId": "71e6ce0c-5471-4883-9c11-7643a60c448e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 1\n",
            "Number of testing samples: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture"
      ],
      "metadata": {
        "id": "xg-7OTQ5riva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator\n",
        "TODO Modify the code"
      ],
      "metadata": {
        "id": "YqZAcAJJSXEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, padding, down=True, use_act=True, **kwargs):\n",
        "        super().__init__()\n",
        "        if down:\n",
        "          self.conv1 = nn.Conv2d(in_channel, out_channel, padding_mode=\"reflect\", kernel_size=kernel_size, padding=padding, **kwargs)\n",
        "        else:\n",
        "          self.conv1 = nn.ConvTranspose2d(in_channel, out_channel, kernel_size=kernel_size, padding=padding, **kwargs)\n",
        "        self.IN = nn.InstanceNorm2d(out_channel)\n",
        "        if use_act:\n",
        "          self.acv = nn.ReLU(inplace=True)\n",
        "        else:\n",
        "          self.acv = nn.Identity()\n",
        "    def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.IN(x)\n",
        "      return  self.acv(x)"
      ],
      "metadata": {
        "id": "KMhkBIsOSWZx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, channels):\n",
        "    super().__init__()\n",
        "    self.Conv1 = ConvBlock(channels, channels, kernel_size=3, padding=1)\n",
        "    self.Conv2 = ConvBlock(channels, channels, kernel_size=3, padding=1, use_act=False)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x2 = self.Conv1(x)\n",
        "    x2 = self.Conv2(x2)\n",
        "    return x + x2"
      ],
      "metadata": {
        "id": "u6cqvJqZuvW1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, img_channels, num_features=64, num_residual=9):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n",
        "    self.acv1 = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.DownBlock = nn.ModuleList([\n",
        "        nn.Conv2d(num_features, num_features*2, kernel_size=3, stride=2, padding=1),\n",
        "        nn.Conv2d(num_features*2, num_features*4, kernel_size=3, stride=2, padding=1),\n",
        "    ])\n",
        "\n",
        "    self.residual_blocks = nn.Sequential(\n",
        "        *[ResBlock(num_features*4) for _ in range(num_residual)]\n",
        "    )\n",
        "\n",
        "    self.up_blocks = nn.ModuleList([\n",
        "        ConvBlock(num_features*4, num_features*2, kernel_size=3, stride=2, padding=1, output_padding=1, down=False),\n",
        "        ConvBlock(num_features*2, num_features*1, kernel_size=3, stride=2, padding=1, output_padding=1, down=False),\n",
        "    ])\n",
        "\n",
        "    self.last_layer = nn.Conv2d(num_features, img_channels, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.acv1(x)\n",
        "    for layer in self.DownBlock:\n",
        "      x = layer(x)\n",
        "\n",
        "    for layer in self.up_blocks:\n",
        "      x = layer(x)\n",
        "    return torch.tanh(self.last_layer(x))\n",
        "    \n"
      ],
      "metadata": {
        "id": "bd4faoHsux50"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator\n",
        "TODO Modify the code"
      ],
      "metadata": {
        "id": "nBs6pDsiSd-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscBlock(nn.Module):\n",
        "  def __init__(self, in_channel, out_channel, stride):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channel, out_channel, 4, stride, 1, bias=True, padding_mode=\"reflect\")\n",
        "    self.IN1 = nn.InstanceNorm2d(out_channel)\n",
        "    self.act1 = nn.LeakyReLU(0.2)\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.IN1(x)\n",
        "    x = self.act1(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "RydVOs8Cgcmm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channel, num_features=[64, 128, 256, 512]):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channel, num_features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\")\n",
        "    self.act1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "    layers = []\n",
        "    in_channel = num_features[0]\n",
        "    \n",
        "    for counter in range(1,len(num_features)):\n",
        "\n",
        "      layers.append(DiscBlock(in_channel, num_features[counter], stride=1 if counter==(len(num_features)-1) else 2))\n",
        "      in_channel = num_features[counter]\n",
        "    layers.append(nn.Conv2d(in_channel, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.act1(x)\n",
        "    x = self.model(x)\n",
        "    return torch.sigmoid(x)\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "M8T-FBTESrPw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CycleGAN"
      ],
      "metadata": {
        "id": "ol9q-KBJcBq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to train CycleGAN for one epoch\n",
        "# Combine generators and discriminators together\n",
        "# Calculate losses and backpropagation\n",
        "def train_CycleGAN_for_one_epoch(G_A, D_B, G_B, D_A,\n",
        "                                 cri_GAN, cri_Cycle, cri_Idt,\n",
        "                                 opt_G, opt_D,\n",
        "                                 data_loader,\n",
        "                                 use_image_pools=False, image_pools=None):\n",
        "  model_name = \"CycleGAN_\" + os.path.basename(DATASET_DIR[:-1])\n",
        "\n",
        "  avg_loss_G = 0\n",
        "  avg_loss_D_B = 0 # Discriminator for A->B\n",
        "  avg_loss_D_A = 0 # Discriminator for B->A\n",
        "\n",
        "  G_A.train()\n",
        "  D_B.train()\n",
        "  G_B.train()\n",
        "  D_A.train()\n",
        "\n",
        "  # labels for discriminators\n",
        "  # shape needs to change as the output of discriminators changes\n",
        "  D_labels_real = torch.ones([BATCH_SIZE,1,30,30]).to(DEVICE)\n",
        "  D_labels_fake = torch.zeros([BATCH_SIZE,1,30,30]).to(DEVICE)\n",
        "\n",
        "  for i, (A_real, B_real) in enumerate(tqdm(data_loader)):\n",
        "    A_real = A_real.to(DEVICE)\n",
        "    B_real = B_real.to(DEVICE)\n",
        "\n",
        "    # forward pass; generate fake images\n",
        "    B_fake = G_A(A_real) # G_A(A)\n",
        "    A_fake = G_B(B_real) # G_B(B)\n",
        "\n",
        "    # if use old generated images to train discriminators\n",
        "    if use_image_pools:\n",
        "      B_fake_for_D = query_image_pool(image_pools['B'], IMAGE_POOL_SIZE,\n",
        "                                      B_fake.detach())\n",
        "      A_fake_for_D = query_image_pool(image_pools['A'], IMAGE_POOL_SIZE,\n",
        "                                      A_fake.detach())\n",
        "      if i == 0:\n",
        "        model_name += \"_ImagePool\"\n",
        "    else:\n",
        "      B_fake_for_D = B_fake.detach()\n",
        "      A_fake_for_D = A_fake.detach()\n",
        "      if i == 0:\n",
        "        model_name += \"_NoImagePool\"\n",
        "\n",
        "    ### Update discriminators\n",
        "    opt_D.zero_grad()\n",
        "    ## Update D_B (discriminator for A->B)\n",
        "    # Real images\n",
        "    loss_D_B_real = cri_GAN(D_B(B_real), D_labels_real)\n",
        "    # Fake images\n",
        "    loss_D_B_fake = cri_GAN(D_B(B_fake_for_D), D_labels_fake)\n",
        "    # Combine losses and compute gradients\n",
        "    loss_D_B = (loss_D_B_real + loss_D_B_fake) * 0.5\n",
        "    loss_D_B.backward()\n",
        "    ## Update D_A (discriminator for B->A)\n",
        "    # Real images\n",
        "    loss_D_A_real = cri_GAN(D_A(A_real), D_labels_real)\n",
        "    # Fake images\n",
        "    loss_D_A_fake = cri_GAN(D_A(A_fake_for_D), D_labels_fake)\n",
        "    # Combine losses and compute gradients\n",
        "    loss_D_A = (loss_D_A_real + loss_D_A_fake) * 0.5\n",
        "    loss_D_A.backward()\n",
        "    # Update the optimizer\n",
        "    opt_D.step()\n",
        "\n",
        "    ### Update generators\n",
        "    opt_G.zero_grad()\n",
        "    # GAN losses\n",
        "    loss_GAN_G_A = cri_GAN(D_B(B_fake), D_labels_real)\n",
        "    loss_GAN_G_B = cri_GAN(D_A(A_fake), D_labels_real)\n",
        "    # Cycle losses\n",
        "    loss_Cycle_G_A = cri_Cycle(G_B(B_fake), A_real) # loss(G_B(G_A(A)), A)\n",
        "    loss_Cycle_G_B = cri_Cycle(G_A(A_fake), B_real) # loss(G_A(G_B(B)), B)\n",
        "    # Identity losses\n",
        "    loss_Idt_G_A = cri_Idt(G_A(B_real), B_real) # loss(G_A(B), B)\n",
        "    loss_Idt_G_B = cri_Idt(G_B(A_real), A_real) # loss(G_B(A), A)\n",
        "    # Combine all losses\n",
        "    loss_G = (loss_GAN_G_A + loss_GAN_G_B\n",
        "              + LAMBDA_CYCLE * (loss_Cycle_G_A + loss_Cycle_G_B\n",
        "                                + LAMBDA_IDT * (loss_Idt_G_A + loss_Idt_G_B)))\n",
        "    # Update\n",
        "    loss_G.backward()\n",
        "    opt_G.step()\n",
        "\n",
        "    avg_loss_G += loss_G.item()\n",
        "    avg_loss_D_B += loss_D_B.item()\n",
        "    avg_loss_D_A += loss_D_A.item()\n",
        "\n",
        "  avg_loss_G = avg_loss_G / len(data_loader)\n",
        "  avg_loss_D_B = avg_loss_D_B / len(data_loader)\n",
        "  avg_loss_D_A = avg_loss_D_A / len(data_loader)\n",
        "\n",
        "  return model_name, (avg_loss_G, avg_loss_D_B, avg_loss_D_A)"
      ],
      "metadata": {
        "id": "2HGs3VEMhiSe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Y_tLfkrbvZp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training setup\n",
        "\n",
        "# Generator and Discriminator models\n",
        "# Mapping A->B\n",
        "G_A = Generator(img_channels=3, num_residual=9).to(DEVICE)\n",
        "D_B = Discriminator(in_channel=3).to(DEVICE)\n",
        "# Mapping B->A\n",
        "G_B = Generator(img_channels=3, num_residual=9).to(DEVICE)\n",
        "D_A = Discriminator(in_channel=3).to(DEVICE)\n",
        "\n",
        "# Initialize the weights of models\n",
        "init_weights(G_A)\n",
        "init_weights(D_B)\n",
        "init_weights(G_B)\n",
        "init_weights(D_A)\n",
        "\n",
        "# Losses\n",
        "cri_GAN = nn.MSELoss() # Least-Squares GAN; see Reference 35 of the paper\n",
        "cri_Cycle = nn.L1Loss()\n",
        "cri_Idt = nn.L1Loss()\n",
        "\n",
        "# Optimizers\n",
        "opt_G = torch.optim.Adam(itertools.chain(G_A.parameters(), G_B.parameters()),\n",
        "                         lr=LR, betas=(0.5, 0.999))\n",
        "opt_D = torch.optim.Adam(itertools.chain(D_B.parameters(), D_A.parameters()),\n",
        "                         lr=LR, betas=(0.5, 0.999))\n",
        "\n",
        "# Image pools\n",
        "image_pools = {'A': [], 'B': []}\n",
        "\n",
        "# Schedulers for learning rate decay\n",
        "def lambda_rule(epoch): # adapted from authors' Github repo \n",
        "  lr_l = 1.0 - max(0, epoch - (NUM_EPOCHS - NUM_EPOCHS_DECAY)) / float(NUM_EPOCHS_DECAY + 1)\n",
        "  return lr_l\n",
        "sclr_G = lr_scheduler.LambdaLR(opt_G, lr_lambda=lambda_rule)\n",
        "sclr_D = lr_scheduler.LambdaLR(opt_D, lr_lambda=lambda_rule)"
      ],
      "metadata": {
        "id": "jm-YnUgjyVR7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for all epoches\n",
        "losses_G = []\n",
        "losses_D_B = []\n",
        "losses_D_A = []\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "  print(\"Epoch: {} / {}\".format(epoch, NUM_EPOCHS))\n",
        "\n",
        "  # Training process\n",
        "  model_name, stats = train_CycleGAN_for_one_epoch(G_A, D_B, G_B, D_A,\n",
        "                                                   cri_GAN, cri_Cycle, cri_Idt,\n",
        "                                                   opt_G, opt_D,\n",
        "                                                   train_data_loader,\n",
        "                                                   use_image_pools=True,\n",
        "                                                   image_pools=image_pools)\n",
        "  sclr_G.step()\n",
        "  sclr_D.step()\n",
        "\n",
        "  losses_G.append(stats[0])\n",
        "  losses_D_B.append(stats[1])\n",
        "  losses_D_A.append(stats[2])\n",
        "\n",
        "  if epoch == NUM_EPOCHS:\n",
        "    print()\n",
        "  print(\"----------------------------------------------------------------------------------\")\n",
        "  print(\"  Loss G: %f, Loss D_B: %f, Loss D_A: %f, Next LR: %f\" % (\n",
        "      stats[0], stats[1], stats[2], sclr_G.get_last_lr()[0]\n",
        "  ))\n",
        "  print(\"----------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "SKNcY7VNyvWu",
        "outputId": "3f93d09c-dd24-4496-eefd-f4c2c650e628",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 / 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------\n",
            "  Loss G: 19.023766, Loss D_B: 0.309831, Loss D_A: 0.311158, Next LR: 0.000100\n",
            "----------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_MODELS:\n",
        "  torch.save(G_A.cpu().state_dict(), MODEL_DIR + model_name + \"_GA.pt\")\n",
        "  torch.save(D_B.cpu().state_dict(), MODEL_DIR + model_name + \"_DB.pt\")\n",
        "  torch.save(G_B.cpu().state_dict(), MODEL_DIR + model_name + \"_GB.pt\")\n",
        "  torch.save(D_A.cpu().state_dict(), MODEL_DIR + model_name + \"_DA.pt\")"
      ],
      "metadata": {
        "id": "tXSWLWFIEJH2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/ECE-GY 6953 DL/Project/SavedModels/\" -l"
      ],
      "metadata": {
        "id": "PBeMH6r_Eon2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e59566-dfae-43fb-cfb7-767985cdc24a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 221082\n",
            "-rw------- 1 root root 11062975 May 13 07:48 CycleGAN_horse2zebra_ImagePool_DA.pt\n",
            "-rw------- 1 root root 11062975 May 13 07:48 CycleGAN_horse2zebra_ImagePool_DB.pt\n",
            "-rw------- 1 root root 45533417 May 13 07:48 CycleGAN_horse2zebra_ImagePool_GA.pt\n",
            "-rw------- 1 root root 45533417 May 13 07:48 CycleGAN_horse2zebra_ImagePool_GB.pt\n",
            "-rw------- 1 root root 11062999 May 12 01:48 CycleGAN_horse2zebra_NoImagePool_DA.pt\n",
            "-rw------- 1 root root 11062999 May 12 01:48 CycleGAN_horse2zebra_NoImagePool_DB.pt\n",
            "-rw------- 1 root root 45533517 May 12 01:48 CycleGAN_horse2zebra_NoImagePool_GA.pt\n",
            "-rw------- 1 root root 45533517 May 12 01:48 CycleGAN_horse2zebra_NoImagePool_GB.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evalutaion"
      ],
      "metadata": {
        "id": "U_XEwmibJQzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Curves"
      ],
      "metadata": {
        "id": "a9b-Id-ZpRCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(losses_G, label=\"Loss G\")\n",
        "plt.plot(losses_D_B, label=\"Loss D_B\")\n",
        "plt.plot(losses_D_A, label=\"Loss D_A\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Losses\")\n",
        "plt.legend()\n",
        "\n",
        "if SAVE_LOSS_PLOTS:\n",
        "  plt.savefig(VISUAL_DIR + model_name + \"_losses.png\")"
      ],
      "metadata": {
        "id": "nTcnIZiLybtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Sample Visualization"
      ],
      "metadata": {
        "id": "ystC1PNWpUNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_real = next(iter(test_data_loader))[0][0] # A\n",
        "print(tmp_real.dtype)\n",
        "plt.imshow(tmp_real.permute(1,2,0))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EnQ_XRto4gqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_A.eval()\n",
        "tmp_fake = G_A(tmp_real.float().to(DEVICE)).cpu().detach()"
      ],
      "metadata": {
        "id": "tYjqBmKY5L1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_fake_post = tmp_fake.cpu().float().numpy()\n",
        "tmp_fake_post = (np.transpose(tmp_fake_post, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "tmp_fake_post = tmp_fake_post.astype(np.uint8)\n",
        "plt.imshow(tmp_fake_post)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V3QiFiTi5SDg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}