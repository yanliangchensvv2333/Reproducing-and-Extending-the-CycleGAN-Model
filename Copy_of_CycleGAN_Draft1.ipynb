{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ],
      "metadata": {
        "id": "TSEh-iK-I7d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dCvQHGCSJfGL",
        "outputId": "2342ed16-4f3c-45f3-ff0b-ae1de84baf3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j-OVhbBGfISI"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.utils import save_image\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "wHGc09O3NG6X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters and Constant Variables"
      ],
      "metadata": {
        "id": "6bVWAUIxVojW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)"
      ],
      "metadata": {
        "id": "-vxmIPcJ3KLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ff50a4-4f9a-4e40-dd16-5b3e2f981639"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DIR = \"/content/drive/MyDrive/cats2/\"\n",
        "TRAIN_A_PATH = DATASET_DIR + \"train/hyena\"\n",
        "TRAIN_B_PATH = DATASET_DIR + \"train/cheetah\"\n",
        "TEST_A_PATH = DATASET_DIR + \"validation/hyena\"\n",
        "TEST_B_PATH = DATASET_DIR + \"validation/cheetah\"\n",
        "\n",
        "MODEL_DIR = \"/content/drive/MyDrive/SavedModels/\"\n",
        "\n",
        "VISUAL_DIR = \"/content/drive/MyDrive/Visualizations/\""
      ],
      "metadata": {
        "id": "LD1k80gisQqY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 200\n",
        "NUM_EPOCHS_DECAY = 100 # the number of last epoches during which lr decays to 0\n",
        "\n",
        "LR = 0.0002\n",
        "LAMBDA_CYCLE = 10.0\n",
        "LAMBDA_IDT = 0.5\n",
        "IMAGE_POOL_SIZE = 50\n",
        "\n",
        "MAX_NUM_SAMPLES = 1000 # if larger than the number of training/testing samples, use all samples\n",
        "NUM_DATALOADER_WORKERS = 2\n",
        "SAVE_MODELS = True\n",
        "SAVE_LOSS_PLOTS = True"
      ],
      "metadata": {
        "id": "lKEvvEdRVn3V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "7XlppgzSw4uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The code in this cell is adapted from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git.\n",
        "\n",
        "# Method to obtain/insert images from/into the image pool\n",
        "def query_image_pool(image_pool, pool_size, curr_images):\n",
        "  if pool_size == 0:\n",
        "      return curr_images\n",
        "  ret_images = []\n",
        "  for image in curr_images:\n",
        "      image = torch.unsqueeze(image.data, 0)\n",
        "      if len(image_pool) < pool_size: # if buffer is not full\n",
        "          image_pool.append(image)\n",
        "          ret_images.append(image)\n",
        "      else: # if buffer is full\n",
        "          # by 50% chance, return an old image from the pool and replace it with\n",
        "          # a current image; otherwise, return the current image\n",
        "          p = random.uniform(0, 1)\n",
        "          if p > 0.5:\n",
        "              random_id = random.randint(0, pool_size - 1)\n",
        "              old_image = image_pool[random_id].clone()\n",
        "              image_pool[random_id] = image\n",
        "              ret_images.append(old_image)\n",
        "          else:\n",
        "              ret_images.append(image)\n",
        "  return torch.cat(ret_images, 0)"
      ],
      "metadata": {
        "id": "rR7l18OIBsCD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code in this cell is adapted from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git.\n",
        "\n",
        "# Method to initialize weights of the given model\n",
        "def init_weights(model):\n",
        "  def init_func(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "      init.normal_(m.weight.data, 0.0, 0.02)\n",
        "      if hasattr(m, 'bias') and m.bias is not None:\n",
        "          init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "      init.normal_(m.weight.data, 1.0, 0.02)\n",
        "      init.constant_(m.bias.data, 0.0)\n",
        "  model.apply(init_func)"
      ],
      "metadata": {
        "id": "XeEC6hCH9A8o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code in this cell is adapted from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git.\n",
        "\n",
        "# Method to post-processing the output image of the generator\n",
        "def post_processing_image(image):\n",
        "  image = image.cpu().float().numpy()\n",
        "  image_np = (np.transpose(image, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "  return image_np.astype(np.uint8)"
      ],
      "metadata": {
        "id": "_0gl56YpxCb0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "xorqpTpAV18P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, A_dir, B_dir, transform, random_pairs=True):\n",
        "        self.A_dir = A_dir\n",
        "        self.B_dir = B_dir\n",
        "        self.transform = transform\n",
        "        self.random_pairs = random_pairs\n",
        "\n",
        "        self.A_img_filenames = os.listdir(self.A_dir)[:MAX_NUM_SAMPLES]\n",
        "        self.B_img_filenames = os.listdir(self.B_dir)[:MAX_NUM_SAMPLES]\n",
        "        self.A_size = len(self.A_img_filenames)\n",
        "        self.B_size = len(self.B_img_filenames)\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(self.A_size, self.B_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        A_img_path = os.path.join(self.A_dir,\n",
        "                                  self.A_img_filenames[index % self.A_size])\n",
        "        if self.random_pairs:\n",
        "          B_index = random.randint(0, self.B_size - 1)\n",
        "        else:\n",
        "          B_index = index % self.B_size\n",
        "        B_img_path = os.path.join(self.B_dir, self.B_img_filenames[B_index])\n",
        "\n",
        "        A_img = self.transform(Image.open(A_img_path).convert(\"RGB\"))\n",
        "        B_img = self.transform(Image.open(B_img_path).convert(\"RGB\"))\n",
        "\n",
        "        return A_img, B_img"
      ],
      "metadata": {
        "id": "TNYKRpXTV6GP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image transformation\n",
        "train_data_transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "test_data_transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "e9J4e-ONnS11"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup datasets and dataloaders\n",
        "train_dataset = CustomDataset(TRAIN_A_PATH, TRAIN_B_PATH, train_data_transform,\n",
        "                              random_pairs=True)\n",
        "test_dataset = CustomDataset(TEST_A_PATH, TEST_B_PATH, test_data_transform,\n",
        "                             random_pairs=False)\n",
        "print(\"Number of training samples:\", len(train_dataset))\n",
        "print(\"Number of testing samples:\", len(train_dataset))\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                batch_size=BATCH_SIZE,\n",
        "                                                shuffle=True,\n",
        "                                                num_workers=NUM_DATALOADER_WORKERS)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               shuffle=False,\n",
        "                                               num_workers=NUM_DATALOADER_WORKERS)"
      ],
      "metadata": {
        "id": "-YQFxRVa3vXe",
        "outputId": "d2a093ec-b8be-47f8-94e8-84e339836556",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 900\n",
            "Number of testing samples: 900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture"
      ],
      "metadata": {
        "id": "xg-7OTQ5riva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator"
      ],
      "metadata": {
        "id": "YqZAcAJJSXEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channel, out_channel, kernel_size, stride, padding, use_leaky=False, use_norm=True, use_acv=True):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
        "    if use_norm:\n",
        "      self.norm = nn.InstanceNorm2d(out_channel)\n",
        "    else:\n",
        "      self.norm = nn.Identity()\n",
        "    if not use_acv:\n",
        "      self.acv = nn.Identity()\n",
        "    elif use_leaky:\n",
        "      self.acv = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "    else:\n",
        "      self.acv = nn.ReLU(inplace=True)\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.norm(x)\n",
        "    return self.acv(x)"
      ],
      "metadata": {
        "id": "KMhkBIsOSWZx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpsamlingBlock(nn.Module):\n",
        "  def __init__(self, in_channel, out_channel, use_dropout=False, p=0.5):\n",
        "    super().__init__()\n",
        "    self.use_dropout = use_dropout\n",
        "    self.conv = nn.ConvTranspose2d(in_channel, out_channel, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "    self.norm = nn.InstanceNorm2d(out_channel)\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.acv = nn.ReLU(inplace=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.norm(x)\n",
        "    if self.use_dropout:\n",
        "      x = self.dropout(x)\n",
        "    return self.acv(x)"
      ],
      "metadata": {
        "id": "KrLh75O6Apfn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, channel, p=0.5):\n",
        "    super().__init__()\n",
        "    self.padding = nn.ReflectionPad2d(1)\n",
        "    self.conv1 = ConvBlock(channel, channel, kernel_size=3, stride=1, padding=0, use_norm=True)\n",
        "    self.conv2 = ConvBlock(channel, channel, kernel_size=3, stride=1, padding=0, use_norm=True, use_acv=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.padding(x)\n",
        "    x1 = self.conv1(x1)\n",
        "    x1 = self.padding(x1)\n",
        "    x1 = self.conv2(x1)\n",
        "    return x + x1"
      ],
      "metadata": {
        "id": "u6cqvJqZuvW1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, in_channel, out_channel, res_block_num=9):\n",
        "    super().__init__()\n",
        "    layers = []\n",
        "    reflect_pad = nn.ReflectionPad2d(3)\n",
        "    layers += [reflect_pad, \n",
        "          ConvBlock(in_channel, 64, kernel_size=7, stride=1, padding=0, use_leaky=False, use_norm=True, use_acv=True),\n",
        "          ConvBlock(64, 128, kernel_size=3, stride=2, padding=1, use_leaky=False, use_norm=True, use_acv=True),\n",
        "          ConvBlock(128, 256, kernel_size=3, stride=2, padding=1, use_leaky=False, use_norm=True, use_acv=True),\n",
        "          ]\n",
        "    for i in range(res_block_num):\n",
        "      layers += [ResBlock(256)]\n",
        "    \n",
        "    layers += [UpsamlingBlock(256, 128, use_dropout=False),\n",
        "          UpsamlingBlock(128, 64, use_dropout=False)\n",
        "          ]\n",
        "    layers += [reflect_pad,\n",
        "          nn.Conv2d(64, out_channel, kernel_size=7, padding=0),\n",
        "          nn.Tanh(),\n",
        "          ]\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "    \n"
      ],
      "metadata": {
        "id": "bd4faoHsux50"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator"
      ],
      "metadata": {
        "id": "nBs6pDsiSd-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channel):\n",
        "    super().__init__()\n",
        "    self.conv1 = ConvBlock(in_channel, 64, kernel_size=4, stride=2, padding=1, use_leaky=True, use_norm=False, use_acv=True)\n",
        "    self.conv2 = ConvBlock(64, 128, kernel_size=4, stride=2, padding=1, use_leaky=True, use_norm=True, use_acv=True)\n",
        "    self.conv3 = ConvBlock(128, 256, kernel_size=4, stride=2, padding=1, use_leaky=True, use_norm=True, use_acv=True)\n",
        "    self.conv4 = ConvBlock(256, 512, kernel_size=4, stride=2, padding=1, use_leaky=True, use_norm=True, use_acv=True)\n",
        "    self.output = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    return self.output(x)\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "M8T-FBTESrPw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CycleGAN"
      ],
      "metadata": {
        "id": "ol9q-KBJcBq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to train CycleGAN for one epoch\n",
        "# Combine generators and discriminators together\n",
        "# Calculate losses and backpropagation\n",
        "def train_CycleGAN_for_one_epoch(G_A, D_B, G_B, D_A,\n",
        "                                 cri_GAN, cri_Cycle, cri_Idt,\n",
        "                                 opt_G, opt_D,\n",
        "                                 data_loader,\n",
        "                                 use_image_pools=False, image_pools=None):\n",
        "  model_name = \"CycleGAN_\" + os.path.basename(DATASET_DIR[:-1])\n",
        "\n",
        "  avg_loss_G = 0\n",
        "  avg_loss_D_B = 0 # Discriminator for A->B\n",
        "  avg_loss_D_A = 0 # Discriminator for B->A\n",
        "\n",
        "  G_A.train()\n",
        "  D_B.train()\n",
        "  G_B.train()\n",
        "  D_A.train()\n",
        "\n",
        "  # labels for discriminators\n",
        "  D_labels_real = None\n",
        "  D_labels_fake = None\n",
        "\n",
        "  for i, (A_real, B_real) in enumerate(tqdm(data_loader)):\n",
        "    A_real = A_real.to(DEVICE)\n",
        "    B_real = B_real.to(DEVICE)\n",
        "\n",
        "    # forward pass; generate fake images\n",
        "    B_fake = G_A(A_real) # G_A(A)\n",
        "    A_fake = G_B(B_real) # G_B(B)\n",
        "\n",
        "    # if use old generated images to train discriminators\n",
        "    if use_image_pools:\n",
        "      B_fake_for_D = query_image_pool(image_pools['B'], IMAGE_POOL_SIZE,\n",
        "                                      B_fake.detach())\n",
        "      A_fake_for_D = query_image_pool(image_pools['A'], IMAGE_POOL_SIZE,\n",
        "                                      A_fake.detach())\n",
        "      if i == 0:\n",
        "        model_name += \"_ImagePool\"\n",
        "    else:\n",
        "      B_fake_for_D = B_fake.detach()\n",
        "      A_fake_for_D = A_fake.detach()\n",
        "      if i == 0:\n",
        "        model_name += \"_NoImagePool\"\n",
        "\n",
        "    ### Update discriminators\n",
        "    opt_D.zero_grad()\n",
        "    ## Update D_B (discriminator for A->B)\n",
        "    # Real images\n",
        "    D_B_real_output = D_B(B_real)\n",
        "    if i == 0: # initialize Ds' labels; only do once\n",
        "      D_labels_real = torch.ones(D_B_real_output.shape).to(DEVICE)\n",
        "      D_labels_fake = torch.zeros(D_B_real_output.shape).to(DEVICE)\n",
        "    loss_D_B_real = cri_GAN(D_B_real_output, D_labels_real)\n",
        "    # Fake images\n",
        "    loss_D_B_fake = cri_GAN(D_B(B_fake_for_D), D_labels_fake)\n",
        "    # Combine losses and compute gradients\n",
        "    loss_D_B = (loss_D_B_real + loss_D_B_fake) * 0.5\n",
        "    loss_D_B.backward()\n",
        "    ## Update D_A (discriminator for B->A)\n",
        "    # Real images\n",
        "    loss_D_A_real = cri_GAN(D_A(A_real), D_labels_real)\n",
        "    # Fake images\n",
        "    loss_D_A_fake = cri_GAN(D_A(A_fake_for_D), D_labels_fake)\n",
        "    # Combine losses and compute gradients\n",
        "    loss_D_A = (loss_D_A_real + loss_D_A_fake) * 0.5\n",
        "    loss_D_A.backward()\n",
        "    # Update the optimizer\n",
        "    opt_D.step()\n",
        "\n",
        "    ### Update generators\n",
        "    opt_G.zero_grad()\n",
        "    # GAN losses\n",
        "    loss_GAN_G_A = cri_GAN(D_B(B_fake), D_labels_real)\n",
        "    loss_GAN_G_B = cri_GAN(D_A(A_fake), D_labels_real)\n",
        "    # Cycle losses\n",
        "    loss_Cycle_G_A = cri_Cycle(G_B(B_fake), A_real) # loss(G_B(G_A(A)), A)\n",
        "    loss_Cycle_G_B = cri_Cycle(G_A(A_fake), B_real) # loss(G_A(G_B(B)), B)\n",
        "    # Identity losses\n",
        "    loss_Idt_G_A = cri_Idt(G_A(B_real), B_real) # loss(G_A(B), B)\n",
        "    loss_Idt_G_B = cri_Idt(G_B(A_real), A_real) # loss(G_B(A), A)\n",
        "    # Combine all losses\n",
        "    loss_G = (loss_GAN_G_A + loss_GAN_G_B\n",
        "              + LAMBDA_CYCLE * (loss_Cycle_G_A + loss_Cycle_G_B\n",
        "                                + LAMBDA_IDT * (loss_Idt_G_A + loss_Idt_G_B)))\n",
        "    # Update\n",
        "    loss_G.backward()\n",
        "    opt_G.step()\n",
        "\n",
        "    avg_loss_G += loss_G.item()\n",
        "    avg_loss_D_B += loss_D_B.item()\n",
        "    avg_loss_D_A += loss_D_A.item()\n",
        "\n",
        "  avg_loss_G = avg_loss_G / len(data_loader)\n",
        "  avg_loss_D_B = avg_loss_D_B / len(data_loader)\n",
        "  avg_loss_D_A = avg_loss_D_A / len(data_loader)\n",
        "\n",
        "  return model_name, (avg_loss_G, avg_loss_D_B, avg_loss_D_A)"
      ],
      "metadata": {
        "id": "2HGs3VEMhiSe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Y_tLfkrbvZp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training setup\n",
        "\n",
        "# Generator and Discriminator models\n",
        "# Mapping A->B\n",
        "G_A = Generator(in_channel=3, out_channel=3).to(DEVICE)\n",
        "D_B = Discriminator(in_channel=3).to(DEVICE)\n",
        "# # Mapping B->A\n",
        "G_B = Generator(in_channel=3, out_channel=3).to(DEVICE)\n",
        "D_A = Discriminator(in_channel=3).to(DEVICE)\n",
        "\n",
        "# Initialize the weights of models\n",
        "init_weights(G_A)\n",
        "init_weights(D_B)\n",
        "init_weights(G_B)\n",
        "init_weights(D_A)\n",
        "\n",
        "# Losses\n",
        "cri_GAN = nn.MSELoss() # Least-Squares GAN; see Reference 35 of the paper\n",
        "cri_Cycle = nn.L1Loss()\n",
        "cri_Idt = nn.L1Loss()\n",
        "\n",
        "# Optimizers\n",
        "opt_G = torch.optim.Adam(itertools.chain(G_A.parameters(), G_B.parameters()),\n",
        "                         lr=LR, betas=(0.5, 0.999))\n",
        "opt_D = torch.optim.Adam(itertools.chain(D_B.parameters(), D_A.parameters()),\n",
        "                         lr=LR, betas=(0.5, 0.999))\n",
        "\n",
        "# Image pools\n",
        "image_pools = {'A': [], 'B': []}\n",
        "\n",
        "# Schedulers for learning rate decay\n",
        "def lambda_rule(epoch): # adapted from authors' Github repo \n",
        "  lr_l = 1.0 - max(0, epoch - (NUM_EPOCHS - NUM_EPOCHS_DECAY)) / float(NUM_EPOCHS_DECAY + 1)\n",
        "  return lr_l\n",
        "sclr_G = lr_scheduler.LambdaLR(opt_G, lr_lambda=lambda_rule)\n",
        "sclr_D = lr_scheduler.LambdaLR(opt_D, lr_lambda=lambda_rule)"
      ],
      "metadata": {
        "id": "jm-YnUgjyVR7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for all epoches\n",
        "losses_G = []\n",
        "losses_D_B = []\n",
        "losses_D_A = []\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "  print(\"Epoch: {} / {}\".format(epoch, NUM_EPOCHS))\n",
        "\n",
        "  # Training process\n",
        "  model_name, stats = train_CycleGAN_for_one_epoch(G_A, D_B, G_B, D_A,\n",
        "                                                   cri_GAN, cri_Cycle, cri_Idt,\n",
        "                                                   opt_G, opt_D,\n",
        "                                                   train_data_loader,\n",
        "                                                   use_image_pools=True,\n",
        "                                                   image_pools=image_pools)\n",
        "  sclr_G.step()\n",
        "  sclr_D.step()\n",
        "\n",
        "  losses_G.append(stats[0])\n",
        "  losses_D_B.append(stats[1])\n",
        "  losses_D_A.append(stats[2])\n",
        "\n",
        "\n",
        "  if epoch % 20 == 0:\n",
        "\n",
        "    pickle_filename = \"/content/drive/MyDrive/loss_\" + model_name + \"_\" + str(epoch)\n",
        "\n",
        "    with open(pickle_filename, \"wb\") as fp:   #Pickling\n",
        "      pickle.dump([losses_G, losses_D_B, losses_D_A], fp)\n",
        "\n",
        "    if SAVE_MODELS:\n",
        "      torch.save(G_A.cpu().state_dict(), MODEL_DIR + model_name + \"_GA_\" + str(epoch)+ \".pt\")\n",
        "      torch.save(D_B.cpu().state_dict(), MODEL_DIR + model_name + \"_DB_\" + str(epoch)+ \".pt\")\n",
        "      torch.save(G_B.cpu().state_dict(), MODEL_DIR + model_name + \"_GB_\" + str(epoch)+ \".pt\")\n",
        "      torch.save(D_A.cpu().state_dict(), MODEL_DIR + model_name + \"_DA_\" + str(epoch)+ \".pt\")\n",
        "\n",
        "  if epoch == NUM_EPOCHS:\n",
        "    print()\n",
        "  print(\"----------------------------------------------------------------------------------\")\n",
        "  print(\"  Loss G: %f, Loss D_B: %f, Loss D_A: %f, Next LR: %f\" % (\n",
        "      stats[0], stats[1], stats[2], sclr_G.get_last_lr()[0]\n",
        "  ))\n",
        "  print(\"----------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "SKNcY7VNyvWu",
        "outputId": "9d22cc6b-3b7b-4f9c-b756-540f824b1920",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 / 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 900/900 [03:00<00:00,  4.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------\n",
            "  Loss G: 7.977016, Loss D_B: 0.327488, Loss D_A: 0.347318, Next LR: 0.000200\n",
            "----------------------------------------------------------------------------------\n",
            "Epoch: 2 / 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 405/900 [01:12<01:26,  5.69it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_MODELS:\n",
        "  torch.save(G_A.cpu().state_dict(), MODEL_DIR + model_name + \"_GA.pt\")\n",
        "  torch.save(D_B.cpu().state_dict(), MODEL_DIR + model_name + \"_DB.pt\")\n",
        "  torch.save(G_B.cpu().state_dict(), MODEL_DIR + model_name + \"_GB.pt\")\n",
        "  torch.save(D_A.cpu().state_dict(), MODEL_DIR + model_name + \"_DA.pt\")"
      ],
      "metadata": {
        "id": "tXSWLWFIEJH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls \"/content/drive/MyDrive/ECE-GY 6953 DL/Project/SavedModels/\" -l"
      ],
      "metadata": {
        "id": "PBeMH6r_Eon2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evalutaion"
      ],
      "metadata": {
        "id": "U_XEwmibJQzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Curves"
      ],
      "metadata": {
        "id": "a9b-Id-ZpRCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(losses_G, label=\"Loss G\")\n",
        "plt.plot(losses_D_B, label=\"Loss D_B\")\n",
        "plt.plot(losses_D_A, label=\"Loss D_A\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Losses\")\n",
        "plt.legend()\n",
        "\n",
        "if SAVE_LOSS_PLOTS:\n",
        "  plt.savefig(VISUAL_DIR + model_name + \"_losses.png\")"
      ],
      "metadata": {
        "id": "nTcnIZiLybtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Sample Visualization"
      ],
      "metadata": {
        "id": "ystC1PNWpUNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this block when loading from saved models\n",
        "# G_A = Generator(in_channel=3, out_channel=3).to(DEVICE)"
      ],
      "metadata": {
        "id": "_NCq0OUaUtm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_real = next(iter(test_data_loader))[0][0] # A\n",
        "plt.imshow(test_real.permute(1,2,0))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EnQ_XRto4gqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_A = G_A.to(DEVICE)\n",
        "G_A.eval()\n",
        "test_fake = G_A(test_real.unsqueeze(0).float().to(DEVICE)).squeeze(0).cpu().detach()"
      ],
      "metadata": {
        "id": "tYjqBmKY5L1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_fake_post = post_processing_image(test_fake)\n",
        "plt.imshow(test_fake_post)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V3QiFiTi5SDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate fake images for multiple test samples\n",
        "G_A = G_A.to(DEVICE)\n",
        "G_B = G_B.to(DEVICE)\n",
        "G_A.eval()\n",
        "G_B.eval()\n",
        "output_dir = VISUAL_DIR + \"GeneratedImages/\"\n",
        "for i, (test_A_real, test_B_real) in enumerate(test_data_loader):\n",
        "  # Test only some samples\n",
        "  if i == 20:\n",
        "    break\n",
        "  \n",
        "  test_A_fake = G_A(test_A_real.float().to(DEVICE))\n",
        "  test_B_fake = G_B(test_B_real.float().to(DEVICE))\n",
        "  save_image(test_A_fake, output_dir + \"A_\" + str(i) + \".png\")\n",
        "  save_image(test_B_fake, output_dir + \"B_\" + str(i) + \".png\")"
      ],
      "metadata": {
        "id": "tGXC4_EhnWyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FCN Scores"
      ],
      "metadata": {
        "id": "R6Xj_9pruOXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://efrosgans.eecs.berkeley.edu/pix2pix/datasets/cityscapes.tar.gz"
      ],
      "metadata": {
        "id": "sfGARmz1uQ1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -zxvf ./cityscapes.tar.gz"
      ],
      "metadata": {
        "id": "X3uXSKlF0349"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(Image.open(\"/content/cityscapes/train/1.jpg\"))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-sJ04YqwzM47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./cityscapes/train/*txt"
      ],
      "metadata": {
        "id": "aQGDAkRy3beo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/phillipi/pix2pix/blob/master/scripts/eval_cityscapes/evaluate.py"
      ],
      "metadata": {
        "id": "bLwM2Eyo6Pqh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}